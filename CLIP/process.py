import argparse
import json
import os
import shutil
from glob import glob

import numpy as np
import pandas as pd
import torch
from PIL import Image
from sentence_transformers import SentenceTransformer, util
from tqdm import tqdm

import settings
# import matplotlib.pyplot as plt
from PIL import Image
import numpy as np


class ImageKeywording:
    """Image Keywording class.

    This class is used for keywording images based on a pre-defined vocabulary.
    It extracts features from both the vocabulary and the images, and assigns best keywords
    to each image based on their features similarity.

    Attributes:
        - keywords_list (list): A list of keywords from a pre-defined vocabulary.
        - image_list (list): A list of image file paths.
        - templated_keywords_list (list): A list of templated keywords generated by replacing a placeholder
        in a prompt templatewith each keyword from the vocabulary.
        - vocab_features (list): Features extracted from the keywords vocabulary.
        - image_features (list): Features extracted from the images.
        - image_keywords (dict): A dictionary mapping each image file path to its assigned keywords.

    Methods:
        - extract_vocab_features(): Extracts features from the keywords vocabulary.
        - extract_image_features(): Extracts features from the images.
        - find_target_synonyms(): Finds synonyms for target keywords based on the vocabulary.
        - get_keywords_for_images(): Assigns best keywords to each image based on their features similarity.
        - save_target_images(): Saves target images in separate folders based on their targer queries.
        - get_images_by_keywords(): Finds best images by target queries.
    """
    def __init__(self, image_list):
        print("__init__")
        with open(settings.VOCABULARY) as f:
            self.keywords_list = json.load(f)
        # self.image_list = glob(settings.IMAGES_DIR + '/**/*.jpg', recursive=True)s
        if image_list == None:
            self.image_list = glob('./data/image' + '/**/*.jpg', recursive=True)
        else:
            self.image_list = image_list

        print(f"{len(self.image_list)} images list in init:", self.image_list)
        print("--------------------------------------------------------------------")
        self.templated_keywords_list = [settings.PROMPT_TEMPLATE.replace('{TARGET}', k) for k in self.keywords_list]
        self.vocab_features = self.extract_vocab_features()
#         self.image_features = self.extract_image_features()
#         self.image_keywords = self.get_keywords_for_images()
        self.image_keywords = self.keywords_extract()
        # self.image_keywords = self.test_extract()

    def extract_vocab_features(self):
        """Extracts features from the vocabulary."""
        print("extract_vocab_features")
        # load data from disk if it has already been saved
        if not os.path.exists(settings.VOCAB_FEATURES):
            text_encoder = SentenceTransformer(settings.TEXT_MODEL, device=settings.DEVICE,
                                               cache_folder=settings.CACHE_FOLDER)
            with torch.no_grad():
                vocab_features = torch.Tensor(text_encoder.encode(self.keywords_list, show_progress_bar=True))
            torch.save(vocab_features, settings.VOCAB_FEATURES)
        else:
            vocab_features = torch.Tensor(torch.load(settings.VOCAB_FEATURES))
        return vocab_features.to(settings.DEVICE)

    def extract_image_features(self):
        """Extracts features from the images."""
        print("extract_image_features")
        # load data from disk if it has already been saved
        if not os.path.exists(settings.IMAGE_FEATURES):
            image_encoder = SentenceTransformer(settings.IMAGE_MODEL, device=settings.DEVICE,
                                                cache_folder=settings.CACHE_FOLDER)

            features, batch = [], []
            for i, img_path in enumerate(tqdm(self.image_list)):
                image = Image.open(img_path).convert('RGB')
                batch.append(image)

                # process each batch
                if (len(batch) >= settings.BATCH_SIZE) or (i == len(self.image_list) - 1):

                    with torch.no_grad():
                        batch_features = image_encoder.encode(batch, show_progress_bar=True)

                    features.extend(batch_features.tolist())
                    batch = []

            image_features = torch.Tensor(features)
            torch.save(image_features, settings.IMAGE_FEATURES)
        else:
            image_features = torch.Tensor(torch.load(settings.IMAGE_FEATURES))
        return image_features.to(settings.DEVICE)

    def find_target_synonyms(self, templated_target_keywords):
        """
        Finds synonyms for target keywords based on the vocabulary.

        Args:
        - templated_target_keywords (list): A list of templated target keywords.

        Returns:
        - matching_dict (dict): A dictionary mapping each target keyword to its synonyms.
        """
        print("find_target_synonyms")
        text_encoder = SentenceTransformer(settings.TEXT_MODEL, device=settings.DEVICE,
                                           cache_folder=settings.CACHE_FOLDER)
        with torch.no_grad():
            target_features = text_encoder.encode(templated_target_keywords, show_progress_bar=True)

        similarity = util.cos_sim(torch.Tensor(target_features).to(settings.DEVICE), self.vocab_features)
        _, indices = similarity.topk(settings.VOCAB_SYNONYMS_COUNT)
        synonyms = np.array(self.keywords_list)[indices.cpu().numpy()]
        matching_dict = {k: synonyms[i].tolist() for i, k in enumerate(settings.TARGET_KEYWORDS)}
        return matching_dict

    def get_keywords_for_images(self):
        """
        Assigns best keywords to each image based on their features similarity.

        Returns:
        - images_keywords (list): A list of assigned keywords for each image.
        """
        print("get_keywords_for_images")
        similarity = util.cos_sim(self.image_features, self.vocab_features)
        _, indices = similarity.topk(settings.TOP_K)
        images_keywords = np.array(self.keywords_list)[indices.cpu().numpy()]
        print(f"{len(indices)} indices: ", indices)
        print("------------------------------------------")

        print("length of images_keywords: ", len(images_keywords))
        print("------------------------------------------")
        # save top_k keywords to dataframe
        if settings.CREATE_KEYWORDING_DF:
            df = pd.concat([pd.DataFrame(data={'images': self.image_list}), pd.DataFrame(images_keywords)], axis=1)
            df.to_csv(os.path.join(settings.CACHE_FOLDER, 'keywords_df.tsv'), sep='\t', index=False)

        return images_keywords

    def save_target_images(self, target_images):
        """
        Saves target images in separate folders based on their targer queries.

        Args:
        - target_images (dict): A dictionary mapping each keyword to a list of target images.
        """
        print("save_target_images")
        os.makedirs(settings.OUTPUT_DIR, exist_ok=True)

        for key, values in tqdm(target_images.items(), desc='Targets'):
            target_folder = os.path.join(settings.OUTPUT_DIR, key)

            if os.path.exists(target_folder):
                shutil.rmtree(target_folder)

            os.mkdir(target_folder)

            for img in values:
                shutil.copy(img, os.path.join(target_folder, os.path.basename(img)))

    def get_images_by_keywords(self):
        """Finds best images by target queries."""
        print("get_images_by_keywords")
        templated_target_keywords = [settings.PROMPT_TEMPLATE.replace('{TARGET}', k) for k in settings.TARGET_KEYWORDS]

        if not settings.VOCAB_SYNONYMS:
            matching_dict = settings.SYNONYMS_DICT
        else:
            matching_dict = self.find_target_synonyms(templated_target_keywords)

        for k, v in matching_dict.items():
            v.append(k)
        
        image_dict = {k: [] for k in matching_dict}
        for key, values in tqdm(matching_dict.items(), desc='Targets', leave=False):
            for img_keywords, image_path in tqdm(zip(self.image_keywords, self.image_list), desc='Keywords'):
                intersection = list(set(values) & set(img_keywords))
                if intersection:
                    image_dict[key].append(image_path)

        self.save_target_images(image_dict)


    def keywords_extract(self):
        """Extracts features from the images."""
        print("keywords_extract")
        # load data from disk if it has already been saved
        image_encoder = SentenceTransformer(settings.IMAGE_MODEL, device=settings.DEVICE,
                                            cache_folder=settings.CACHE_FOLDER)

        features, batch = [], []
        # if image_list==None:
        #     image_list = glob('./data/image' + '/**/*.jpg', recursive=True)
        for i, img_path in enumerate(tqdm(self.image_list)):
            image = Image.open(img_path).convert('RGB')
            batch.append(image)

            # process each batch
            if (len(batch) >= settings.BATCH_SIZE) or (i == len(self.image_list) - 1):

                with torch.no_grad():
                    batch_features = image_encoder.encode(batch, show_progress_bar=True)

                features.extend(batch_features.tolist())
                batch = []

        image_features = torch.Tensor(features)
        image_features = image_features.to(settings.DEVICE)
        # torch.save(image_features, settings.IMAGE_FEATURES)
        # vocab_features.to(settings.DEVICE)

        similarity = util.cos_sim(image_features, self.vocab_features)
        _, indices = similarity.topk(settings.TOP_K)
        images_keywords = np.array(self.keywords_list)[indices.cpu().numpy()]
        # save top_k keywords to dataframe
#         df = pd.concat([pd.DataFrame(data={'images': self.image_list}), pd.DataFrame(images_keywords)], axis=1)
#         output_dir = "keywords_extractions.csv"
#         df.to_csv(output_dir)
#         print("file save to ", output_dir)
        
        # Get the predicted vocab embeddings (top-k) per image
        topk_embeddings = []
        for i in range(indices.shape[0]):
            emb_list = []
            for j in range(indices.shape[1]):
                idx = indices[i][j].item()
                emb = self.vocab_features[idx].cpu().numpy()
                emb_list.append(emb)

                # Print embedding size
#                 print(f"Image {i}, Keyword {j}, Embedding shape: {emb.shape}")
            topk_embeddings.append(emb_list)

        # Save top_k keywords and their embeddings to dataframe
        image_col = pd.DataFrame(data={'images': self.image_list})
        keywords_col = pd.DataFrame(images_keywords)

        # Flatten embedding vectors as strings (or use json.dumps)
        embedding_col = pd.DataFrame([
            [",".join(map(str, emb)) for emb in emb_list] for emb_list in topk_embeddings
        ])

        # Concatenate everything
        df = pd.concat([image_col, keywords_col, embedding_col], axis=1)
        df.columns = ['images'] + [f'keyword_{i+1}' for i in range(settings.TOP_K)] + [f'embedding_{i+1}' for i in range(settings.TOP_K)]

        # Save to CSV
        output_dir = "output/keywords_extractions_with_embeddings.csv"
        df.to_csv(output_dir, index=False)
        print("file saved to", output_dir)

        # print(df)
### FOR PLOTTING
#         num_images = 10
#         num_images = max(num_images, len(self.image_list))  # S'assure qu'on ne dépasse pas la liste
    
#         fig, axes = plt.subplots(num_images, 1, figsize=(10, num_images * 3))  # Grille (num_images, 2)

#         for i in range(num_images):
#             print(df[df.columns[2:]].iloc[i].dropna().tolist())
#             image = Image.open(df["images"][i])

#             # Création de la figure
#             fig, ax = plt.subplots(figsize=(8, 4))  # Taille ajustée
            
#             # Affichage de l'image
#             ax.imshow(image)
#             ax.axis("off")  # Masque les axes

#             # Ajout des mots-clés à droite
#             keywords = "\n".join(df[df.columns[2:]].iloc[i].dropna().tolist())  # Récupère les mots-clés sans NaN
#             plt.text(
#                 image.width + 10, image.height // 2,  # Position à droite de l’image
#                 keywords,
#                 fontsize=12, verticalalignment="center", fontweight="bold"
#             )

#             # Affichage
#             plt.show()


        return df

    def test_extract(self):

        """Extracts features from the images."""
        print("testing_keywords_extract")
        # load data from disk if it has already been saved
        image_encoder = SentenceTransformer(settings.IMAGE_MODEL, device=settings.DEVICE,
                                            cache_folder=settings.CACHE_FOLDER)

        features, batch = [], []
        # if image_list==None:
        #     image_list = glob('./data/image' + '/**/*.jpg', recursive=True)
        for i, img_path in enumerate(tqdm(self.image_list)):
            image = Image.open(img_path).convert('RGB')
            batch.append(image)

            # process each batch
            if (len(batch) >= settings.BATCH_SIZE) or (i == len(self.image_list) - 1):

                with torch.no_grad():
                    batch_features = image_encoder.encode(batch, show_progress_bar=True)

                features.extend(batch_features.tolist())
                batch = []

        image_features = torch.Tensor(features)
        image_features = image_features.to(settings.DEVICE)
        # torch.save(image_features, settings.IMAGE_FEATURES)
        # vocab_features.to(settings.DEVICE)

        similarity = util.cos_sim(image_features, self.vocab_features)
        _, indices = similarity.topk(settings.TOP_K)
        images_keywords = np.array(self.keywords_list)[indices.cpu().numpy()]

        # Get the predicted vocab embeddings (top-k) per image
        topk_embeddings = []
        for i in range(indices.shape[0]):
            emb_list = []
            for j in range(indices.shape[1]):
                idx = indices[i][j].item()
                emb = self.vocab_features[idx].cpu().numpy()
                emb_list.append(emb)

                # Print embedding size
#                 print(f"Image {i}, Keyword {j}, Embedding shape: {emb.shape}")
            topk_embeddings.append(emb_list)

        # Save top_k keywords and their embeddings to dataframe
        image_col = pd.DataFrame(data={'images': self.image_list})
        keywords_col = pd.DataFrame(images_keywords)

        # Flatten embedding vectors as strings (or use json.dumps)
        embedding_col = pd.DataFrame([
            [",".join(map(str, emb)) for emb in emb_list] for emb_list in topk_embeddings
        ])

        # Concatenate everything
        df = pd.concat([image_col, keywords_col, embedding_col], axis=1)
        df.columns = ['images'] + [f'keyword_{i+1}' for i in range(settings.TOP_K)] + [f'embedding_{i+1}' for i in range(settings.TOP_K)]

        for i, row in df.iterrows():
            print(f"\nImage {i+1} Path: {row['images']}")
            for k in range(settings.TOP_K):
                print(f"  Keyword {k+1}: {row[f'keyword_{k+1}']}")
                print(f"  Embedding {k+1}: {row[f'embedding_{k+1}']}")

        
                # Save to CSV
        output_dir = "output/test_embeddings.csv"
        df.to_csv(output_dir, index=False)
        print("file saved to", output_dir)
        
        return df


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-mode', choices=['image_keywording', 'image_search'],
                        default='image_keywording', help='Task type')
    args = parser.parse_args()

    images = glob('./data/images' + '/**/*.jpg', recursive=True)
    image_keywording = ImageKeywording(image_list=images)

    # image_keywording.keywords_extract()
    # if necessary, start an image search
    # if args.mode == 'image_search':
    #     image_keywording.get_images_by_keywords()

    # keywords = image_keywording.get_keywords_for_images()
    # print(f"list of the {len(keywords)}  keywords: ", keywords)